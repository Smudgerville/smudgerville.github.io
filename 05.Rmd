---
title: "Introductory Time Series"
subtitle: "Session 5"
author: "Richard M. Smith"
date: "2018/01/30"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```


---
# Class Overview

- Project Discussion

  - Data Sources

- Homework

- Presentations Methods

- Missing Values in Time Series

- ETS, ARIMA, Dynamic Regression

---
# Homework

Submit as a presentation (Powerpoint, R Presentation, Xaringan) with source R code

3. Use the `usmelec` dataset showing electricity monthly total net generation. January 1973 - June 2013.  Plot the data.  Describe which decomposition method you would use and the features of the data influencing your choice.

4.  Call the `acf` function on the residual series from your chosen decomposition method in question 3.  Describe whether you think your residual series is stationary and why.

5.  Run the Augmented Dickey-Fuller (ADF) test in the `tseries` package on your residual series.  Describe the result in terms of the p-value and null hypothesis.


---
# Presentation Methods

- R Presentation

- Xaringan

- Powerpoint

---
# R Presentation

- Author with Markdown

- Custom CSS

- Host on [rpubs](rpubs.com)

---
# Xaringan

- [Example Slides](https://slides.yihui.name/xaringan/#1)

- Author with Markdown

- Include HTML widgets

- [remark.js](https://remarkjs.com/#1)


---
# Officer

- [Officer Package Site](https://davidgohel.github.io/officer/)

- Can be combined with [mschart](https://ardata-fr.github.io/mschart/articles/introduction.html) for native office charts

- Especially helpful to programmatically create recurring presentations

---
# Missing Values & NA

`zoo` package for irregular time series

- `na.aggregate`

- `na.approx`

- `na.fill`

- `na.locf`

- `na.spline`


---
# ETS vs. ARIMA

- ETS - (Error, Trend, Seasonal) or ExponenTial Smoothing

- ARIMA - Autoregressive integrated moving average

- While exponential smoothing based on description of the trend and seasonality in the data, ARIMA models aim to describe the autocorrelations


---
# ETS and ARIMA in Context

- ARIMA and ETS allow for inclusion of information from past observations of a series

- Regression models allow for inclusion of external variables that explain some of the historical variation and allow for more accurate forecasts


- Combining:

  - Regression with ARIMA errors
  
  - Dynamic Regression Models


---
# Simple Exponential Smoothing

Forecasts produced using exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get older.

- `forecast::ses`

- Appropriate for forecasting data with no clear trend or seasonal pattern

---
# Trend Methods

- Holt's linear trend

- `forecast::holt`

- The forecasts generated by Holt’s linear method display a constant trend (increasing or decreasing) indefinitely into the future. Empirical evidence indicates that these methods tend to over-forecast, especially for longer forecast horizons.

---
# Holt-Winters

- Extended trend method to capture seasonality

- Exponentially weighted moving averages to update estimates of the seasonally adjusted mean, slope, and seasonals

- `forecast::hw`

- `stats::HoltWinters`

---
# Exponential Smoothing Methods

Exponential smoothing methods  are labelled by a pair of letters (T, S) defining the type of 'Trend' and 'Seasonal' components:

| Trend | Seasonal | Seasonal | Seasonal |
| ----- | -------- | -------- | -------- |
| | N (None) | A (Additive) | M (Multiplicative) |
| N (None) | (N, N) | (N, A) | (N, M) |
| A (Additive) | (A, N) | (A, A) | (A, M) |
| A_d (Additive damped) | (A_d, N) | (A_d, A) | (A_d, M) |


---
# Commonly Used Methods

| Notation | Method |
| -------- | ------ |
| (N, N) | Simple exponential smoothing |
| (A, N) | Holt's linear method |
| (A, A) | Additive Holt Winter's |
| (A, M) | Multiplicative Holt Winter's |
| (A_d, M) | Holt-Winters' damped method |

---
# State Space Models

- Exponential smoothing methods discussed so far provide point forecasts

- State space models describe how the unobserved components (level, trend, seasonal) change over time

- These statistical models provide a way to generate an entire forecast distribution a framework for objective model selection

- Notation:  

$ETS(\cdot,\cdot,\cdot)$ (Error, Trend, Seasonal)

---
# `ets` function

- Unlike the `ses`, `holt`, `hw` functions, `ets` does not produce forecasts

- Esimates model parameters and returns information

- ETS point forecasts are equal to the medians of the forecast distributions. For models with only additive components, the forecast distributions are normal, so the medians and means are equal. For ETS models with multiplicative errors, or with multiplicative seasonality, the point forecasts will not be equal to the means of the forecast distributions.

- Model selection minimizes the AICc

---
# Prediction Intervals

Assuming forecast errors are normally distributed, a 95% prediction interval for the $h$-step forecast is:

$$\hat{y}_{T + h|T} \pm 1.96\hat{\sigma}_h$$
Generally:

$$\hat{y}_{T + h|T} \pm k\hat{\sigma}_h$$

Where $k$ depends on the coverage probability

---
# n-Step Prediction intervals

- When forecasting one step ahead, the standard deviation of the forecast distribution is almost the same as the standard deviation of the residuals

- For multi-step forecasts, a more complicated method is required

  - Benchmark Methods
  
  - Bootstrapped Residuals
    
    - Only assumes forecast errors are uncorrelated, not normally distributed


---
# Revisiting Stationarity

A stationary time series is one whose properties do not depend on the time at which the series is observed.

---
# Random Walk Models

Random walks typically have:

1.  long periods of apparent trends up or down

2.  sudden and unpredictable changes of direction

- Forecasts from a random walk are equal to the last observation as future observations are equally likely to be up or down.

- Random walk describes the differenced series

--
## Random Walk with drift

- Differences have a non-zero mean

$$y_t = c + y_{t-1} + e_t$$

If $c$ is positive the series will tend to drift up, if negative it will drift down.


---
# Autoregressive Models

Time series $\{x_t\}$ is an autoregressive process of order $p$, abbreviated to $AR(p)$, if:

$$x_t = c + \alpha_{1}x_{t-1} + \alpha_{2}x_{t-2} + ... + \alpha_{p}x_{t-p} + w_t$$

- Random walk is special case $AR(1)$ with $\alpha_1 = 1$

---
# Notes on AR Models

- When $\alpha_1 = 0$, $y_t$ is equivalent to white noise

- When $\alpha_1 = 1$ and $c = 0$, $y_t$ is equivalent to random walk

- When $\alpha_1 = 1$ and $c \neq 0$, $y_t$ is equivalent to a random walk with drift

- Exponential smoothing model is the special case $\alpha_i = \alpha(1 - \alpha)^i$ for $i = 1, 2, ...$ and $p \rightarrow \infty$

---
# Moving Average Models

- Rather than using past values of the forecast variable in a regression, a moving average model uses past forecast errors in a regression-like model.

- Moving average models should not be confused with the moving average smoothing 

- A moving average model is used for forecasting future values, while moving average smoothing is used for estimating the trend-cycle of past values.

---
# Non-Seasonal ARIMA Models

- Combine differencing with autoregression and a moving average model, we obtain a non-seasonl ARIMA model.

- The *I* is Integrated - in this context, the reverse of differencing

$$ARIMA(p, d, q)$$
- $p =$ order of the autoregressive part

- $d =$ order of the differencing involved

- $q =$ order of the moving average part

---
# Common Examples

| Description | Definition |
| ----------- | ---------- |
| White noise | ARIMA(0, 0, 0) |
| Random Walk | ARIMA(0, 1, 0) with no constant |
| Random Walk with Drift | ARIMA(0, 1, 0) with constant |
| Autoregression | ARIMA(p, 0, 0) |
| Moving Average | ARIMA(0, 0, q) |


---
# General ARIMA Approach

When fitting an ARIMA model to a set of (non-seasonal) time series data, the following procedure provides a useful general approach.

1.  Plot the data and identify any unusual observations.
2.  If necessary, transform the data (using a Box-Cox transformation) to stabilize the variance.
3.  If the data are non-stationary, take first differences of the data until the data are stationary.
4.  Examine the ACF/PACF: Is an ARIMA(p,d,0) or ARIMA(0,d,q) model appropriate?
5.  Try your chosen model(s), and use the AICc to search for a better model.
6.  Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals. If they do not look like white noise, try a modified model.
7.  Once the residuals look like white noise, calculate forecasts.

`auto.arima` only takes care of steps 3–5. So even if you use it, you will still need to take care of the other steps yourself. 

---
# Regression & Generalized Least Squares

- With positive serial correlation in the residual series, implies that standard errors of the estimated regression parameters are likely to be underestimated.  

- Less accurate for the same sample size.  

- GLS can be used to provide better estimates of the standard errors of the regression parameters to account for the autocorrelartion in the residual series.

- `gls`
