---
title: "Introductory Time Series"
subtitle: "Session 3"
author: "Richard M. Smith"
date: "2018/01/23"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

---
# Class Overview

- Project Groups

- R Notebook & Quiz Review

- Decomposition, Differencing, Correlation, Continued

- Git basics

- Forecasting Strategies
  
  - Chapter 3 - Main text
  
  - Chapter 3 - fpp

---
# R Notebook Revisited

- Useful for exploratory analysis - document your thoughts!

- Sharing in a standard format

- Benefits of web-based presentation

--
# Class Reading

- [The State of Being Stuck](https://mathwithbaddrawings.com/2017/09/20/the-state-of-being-stuck/)

- [Rubber Duck Problem Solving](https://blog.codinghorror.com/rubber-duck-problem-solving/)


---
# Class 2 Quiz Questions 1-3


1.  Create a time series object of 80 random normal observations with mean of 10, and standard deviation of 5.  The time series object should be monthly starting in January 1950.  Save this object as a variable named `q1_ts` and plot it.

2.  Using the `q1_ts` data from question 1, compute 3-period and 5-period, 2-sided moving averages on your dataset.  Save them as variables `x3_MA` and `x5_MA` then plot the original series with both 3-period (blue) and 5-period (red) moving average lines.  Include an appropriate title to your plot.

3. Using decomposition, explain whether you would use an additive or multiplicative model for the `nottem` dataset.  Explain why and show any work.

---
# Time Series Decomposition

Additive vs. Multiplicative - does variance increase with time

- Additive:  $y_t = S_t + T_t + R_t$

- Multiplicative:  $y_t = S_t \times T_t \times R_t$

Alternative to using a multiplicative model is to transform the data until variation appears stable over time.

---
# Other widely used decomposition methods

- X11
  - US Census Bureau
  - Trend-cycle available for all observations
  - Seasonal component can vary slowly over time

- SEATS
  - Seasonal Extraction in ARIMA Time Series
  - Only works with quarterly and monthly data

- STL
  - Seasonal and Trend Decomposition using Loess
  - Handles many types of seasonality well
  - Seasonal change and smoothness can be controlled by user
  - Only provides facilities for additive decomposition

---
# Forecasting Decomposition

- Decomposition can be useful in forecasting as well as studying historical data

- Often assumed that the seasonal component is unchanging or changing very slowly
  - Seasonal naive is appropriate

- Seasonally adjusted component can be forecasted with any non-seasonal forecasting method (Random walk with drift, Holt-Winters, Non-seasonal ARIMA)

---
# Expectation

Expected value, commonly abbreviated to $expectation, E$, of a value is its *mean* value in a population.  So $E(x)$ is the mean of $x$, denoted $\mu$
![bike paths](03_files/figure-html/bike_paths.PNG)

[Picture Source](http://paradise.caltech.edu/~cook/papers/TwoNeurons.pdf)

---
# Covariance

Covariance is a measure of *linear association*

$$ \gamma(x, y) = E[(x - \mu_x)(y - \mu_y)] $$
Sample covariance

$$Cov(x, y) = \sum(x_i - \bar{x})(y_i - \bar{y})/(n - 1)$$


---
# Correlation

Dimensionless measure of the linear association between a pair of vairables.

Correlation takes a value between -1 and +1, with a value of 0 indicating no *linear* association.

Population correlation, $\rho$, between a pair of variables $(x, y)$ is:

$$\rho(x, y) = {{E[(x - \mu_x)(y - \mu_y)]} \over {\sigma_x \sigma_y}} = {\gamma(x, y) \over {\sigma_x \sigma_y}}$$

The sample correlation, $Cor$, is an estimate of $\rho$ and is calculated by:

$$Cor(x, y) = {{Cov(x, y)} \over {sd(x)sd(y)}} $$

---
# Correlation

`cor`

```{r}
base_url <- "https://raw.githubusercontent.com/Smudgerville/IntroTimeSeriesWithR/master/"
cbe <- read.table(paste0(base_url, "cbe.dat"), header = T)
with(cbe, cor(choc, elec))
```

We've all heard it before but important to emphasize - correlation does not imply causality

---
# The ensemble and stationarity

Mean function of a time series model:  $u(t) = E(x_t)$

- Expectation is an average across the *ensemble* of all possible series produced by the model. (simulation)

- If the mean function is constant, we say that the time series model is *stationary* in the mean.

- Said another way, a stationary time series is one whose properties do not depend on the time at which the series is observed.

- In general, a stationary time series will have no predictable patterns in the long-term. Time plots will show the series to be roughly horizontal (although some cyclic behaviour is possible), with constant variance.

---
# Variance function

Variance function of a time series model that is stationary in the mean is

$$\sigma^2(t) = E[(x_t - \mu)^2]$$

- This can in theory take a different value at every point in time.  

- However, we can't estimate variance from a single point.  

- Assumption is that model is stationary in the variance.

- In time series, sequential observations may be correlated.  With positive correlation, our model will tend to underestimate in a short series because successive observations are influenced by prior observations.

---
# Differencing

- Differencing is one way to make a time series stationary.

- Transformations such as logarithms can help to stabilize the variance of a time series.  

- Differencing can help stabilize the mean of a time series by removing changes in the level.


---
# Autocorrelation

With time series second-order properties play an important role

Serial correlation or Autocorrelation:  correlation of a variable with itself at different times.  

*Second-order stationary*: autocorrelation depends only on the number of steps between observations.

If a series is *second-order stationary*, then we can define an $autocovariance function (acvf), \gamma_k$, as a function of the lag $k$:

$$\gamma_k = E[(x_t - \mu)(x_{t+k} - \mu)]$$
Simiar to above relationship to covariance but replacing $y$ with $x_{t+k}$, the lag $k$ autocorrelation function $(acf), \rho_k$ is:

$$\rho_k = {{\gamma_k} \over {\sigma^2}}$$

Note:  general discussion will just refer to second-order as stationary.  

Similar to correlation, formula bounds possible values between -1 and 1.

---
# The correlogram

`acf` produces a plot by default.  Underneath it is a list.

Plot type is called correlogram.  Key features:
- *x*-axis give the lag $(k)$.  Unit of lag is sampling interval
- Lag 0 is always 1 and is shown to help comparisons
- If $\rho_k = 0$, the sampling distribution of $r_k$ is approximately normal, with a mean of $-1/n$ and a variance of $1/n$.  Dotted lines are drawn at:

$$-{1 \over n} \pm {2 \over \surd{n}}$$

---
# Correlogram 

- ACF plot is useful for identifying non-stationary time series

- For a stationary time series, the ACF will drop to zero relatively quickly

- The ACF of non-stationary data decreases slowly.

---
## Second-order differencing

Occasionally the differenced data will not appear to be stationary and it may be necessary to differentiate a second time to obtain a stationary series.

--
## Seasonal differencing

A seasonal difference is the difference between an observation and the previous observation from the same season.

$$y^*_t = y - y_{t-1}$$

where $m$ = the number of seasons.  Also called "lag-$m$ differences".  If seasonally differenced data appear to be white noise, then an appropriate model is:

$$y_t = y_{t-m} + e_t$$


---
## Tests for stationarity

- *unit root test*

- *Augmented Dickey-Fuller (ADF) test*

The null-hypotheses for an ADF is that the data are non-stationary.  Thus, small p-values suggest stationarity. 

- `adf.test` in *tseries* package


---
# Git

.pull-left[
- What is git?

- Why is version control important?

- Git vs. Github
]

.pull-right[
![version control](03_files/figure-html/version_control.PNG)
]

---
# Git Resources

- [Main site & documentation](https://git-scm.com/)

  - [Getting Started - Git Basics](https://git-scm.com/book/en/v2/Getting-Started-Git-Basics)

- [Happy Git and GitHub for the useR](http://happygitwithr.com/)

- [Oh shit, git!](http://ohshitgit.com/)


---
# Basic Commands

- `git init`

- `git status`

- `git add .`

- `git commit`
  - `-m "Commit message"`

- `git reflog`
  - `git reset HEAD@{index}`

---
# Key Principles of Forecasting

1.  Set the forecast task clearly & concisely

2.  Implement a systematic approach

3.  Document and justify

  - Seek critical feedback

4.  Segregate forecasters and users

---
# Homework 

Questions 4 & 5 from the in-class quiz in session 2.  Submit as an R Notebook html output.

4.  Describe the difference between the covariance of a sample and the covariance of a population.  Does the `cov` function in R compute a sample or population covariance?

5.  Use the `swiss` data set.  Calculate the correlation between Catholicism and Fertility. Calculate the correlation between Agriculture and Fertility.  Describe what the two numbers tell you. (`?swiss` provides a description of the dataset)